{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/William-Metz/Baseball_Transformer/blob/main/Baseball_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uGSQnfCg8ut"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcd5IT0UdgA0"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COdz0QR96FUI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "781qmTIig3EG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Transformer\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rixOJOwedbJe"
      },
      "source": [
        "##Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpZVMfGdEjQ",
        "outputId": "d3424dca-42ac-475b-b0d5-ecdf4862edd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLCuXW8HddYw"
      },
      "source": [
        "##Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIjv7JAodl_R"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/BaseballNets/pitches.csv')\n",
        "df_AB = pd.read_csv('/content/drive/My Drive/BaseballNets/atbats.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei-XkjvDdjAX"
      },
      "source": [
        "## Data Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcmidHYefo9g"
      },
      "source": [
        "### Merges PitchData with AB data to get pitcher_id and batter_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox2Rn3eVeXL2"
      },
      "outputs": [],
      "source": [
        "df =df.merge(df_AB, on='ab_id', how='left')\n",
        "df.drop(['spin_rate', 'spin_dir', 'break_angle', 'break_length', 'break_y', 'ax', 'ay', 'az', 'sz_bot', 'sz_top', 'type_confidence', 'vx0', 'vy0', 'vz0', 'x', 'x0', 'y', 'y0', 'z0', 'pfx_x', 'pfx_z', 'nasty', 'code', 'zone', 'px', 'pz', 'o','stand', 'g_id', 'top', 'event_num', 'ab_id' ], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM18Ou6eu4KD"
      },
      "source": [
        "### Split and Encode data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPPvsN8Oj7Jk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e349231-936c-4bef-ad96-eeee05dafb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       pitch_type      event type\n",
            "0              FF     Flyout    X\n",
            "1              FF     Flyout    C\n",
            "2              SL     Flyout    S\n",
            "3              CH     Flyout    B\n",
            "4              CH     Flyout    B\n",
            "...           ...        ...  ...\n",
            "387962         CU  Strikeout    W\n",
            "387963         KC    Pop Out    B\n",
            "387964         KC    Pop Out    B\n",
            "387965         FC    Pop Out    B\n",
            "387966         FC    Pop Out    C\n",
            "\n",
            "[383774 rows x 3 columns]\n",
            "        b_score  b_count  s_count  outs  pitch_num  on_1b  on_2b  on_3b  \\\n",
            "0             0        0        0     0          1      0      0      0   \n",
            "1             0        0        0     1          1      0      0      0   \n",
            "2             0        0        0     1          2      0      0      0   \n",
            "3             0        0        1     1          3      0      0      0   \n",
            "4             0        1        1     1          4      0      0      0   \n",
            "...         ...      ...      ...   ...        ...    ...    ...    ...   \n",
            "387962        6        2        1     2          5      0      0      0   \n",
            "387963        7        0        0     0          1      0      0      0   \n",
            "387964        7        1        0     0          2      0      0      0   \n",
            "387965        7        2        0     0          3      0      0      0   \n",
            "387966        7        3        0     0          4      0      0      0   \n",
            "\n",
            "        inning  p_score  batter_id  pitcher_id p_throws  \n",
            "0          1.0      0.0     594777      571666        R  \n",
            "1          1.0      0.0     545361      571666        R  \n",
            "2          1.0      0.0     545361      571666        R  \n",
            "3          1.0      0.0     545361      571666        R  \n",
            "4          1.0      0.0     545361      571666        R  \n",
            "...        ...      ...        ...         ...      ...  \n",
            "387962     7.0      7.0     642133      598264        R  \n",
            "387963     8.0      6.0     456488      592468        R  \n",
            "387964     8.0      6.0     456488      592468        R  \n",
            "387965     8.0      6.0     456488      592468        R  \n",
            "387966     8.0      6.0     456488      592468        R  \n",
            "\n",
            "[383774 rows x 13 columns]\n",
            "(383774, 72)\n",
            "(383774, 2)\n"
          ]
        }
      ],
      "source": [
        "# Check for null values in the continuous columns and handle them if necessary\n",
        "df.dropna(subset=['start_speed', 'end_speed'], inplace=True)\n",
        "\n",
        "# Normalize Pitch Speed\n",
        "scaler = StandardScaler()\n",
        "df[['start_speed', 'end_speed']] = scaler.fit_transform(df[['start_speed', 'end_speed']])\n",
        "\n",
        "# Select columns for features and targets using double brackets for a list of column names\n",
        "feature_columns = df[['b_score', 'b_count', 's_count', 'outs', 'pitch_num', 'on_1b', 'on_2b', 'on_3b', 'inning', 'p_score', 'batter_id', 'pitcher_id', 'p_throws']]\n",
        "categorical_targets = df[['pitch_type', 'event', 'type']]  # These are categorical\n",
        "continuous_targets = df[['start_speed', 'end_speed']]  # These are continuous\n",
        "print(categorical_targets)\n",
        "print(feature_columns)\n",
        "# Create dummy variables for categorical features\n",
        "feature_columns = pd.get_dummies(feature_columns, columns=['pitcher_id', 'batter_id', 'p_throws'])\n",
        "\n",
        "# Create integer codes for categorical targets\n",
        "categorical_targets = pd.get_dummies(categorical_targets)\n",
        "\n",
        "print(categorical_targets.shape)\n",
        "print(continuous_targets.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDMG3zDbg_vk"
      },
      "source": [
        "### Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O46dkzzGe0sT"
      },
      "outputs": [],
      "source": [
        "class PitchDataset(Dataset):\n",
        "    def __init__(self, features, targets, num_categorical):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        self.num_categorical = num_categorical\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract categorical targets (ensure this slice is correct)\n",
        "\n",
        "        categorical_targets = self.targets[idx, :self.num_categorical]\n",
        "        # Extract continuous targets (ensure this slice is correct)\n",
        "        continuous_targets = self.targets[idx, self.num_categorical:]\n",
        "\n",
        "        # Return the feature vector, and a tuple of categorical and continuous targets\n",
        "\n",
        "        return self.features[idx], (categorical_targets, continuous_targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAJJ6D13zHHB"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x )\n",
        "\n",
        "# Modify the Transformer model to have separate outputs for categorical and continuous targets\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes_pitch_type, num_classes_event, num_classes_type, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "\n",
        "        # Transformer Encoder (shared)\n",
        "        self.transformer_encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(self.transformer_encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # Input linear layer to match the input size to d_model\n",
        "        self.input_linear = nn.Linear(input_size, d_model)\n",
        "\n",
        "        # Output linear layers for categorical targets\n",
        "        self.output_linear_pitch_type = nn.Linear(d_model, num_classes_pitch_type)\n",
        "        self.output_linear_event = nn.Linear(d_model, num_classes_event)\n",
        "        self.output_linear_type = nn.Linear(d_model, num_classes_type)\n",
        "\n",
        "        # Output linear layer for continuous targets (2 for start_speed and end_speed)\n",
        "        self.output_linear_continuous = nn.Linear(d_model, 2)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        # Initialize input linear layer\n",
        "        self.input_linear.bias.data.zero_()\n",
        "        self.input_linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Initialize output linear layers for categorical targets\n",
        "        self.output_linear_pitch_type.bias.data.zero_()\n",
        "        self.output_linear_pitch_type.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output_linear_event.bias.data.zero_()\n",
        "        self.output_linear_event.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output_linear_type.bias.data.zero_()\n",
        "        self.output_linear_type.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "        # Initialize output linear layer for continuous targets\n",
        "        self.output_linear_continuous.bias.data.zero_()\n",
        "        self.output_linear_continuous.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.input_linear(src)  # Adjust input to d_model size\n",
        "        src = self.pos_encoder(src)\n",
        "        transformer_output = self.transformer_encoder(src)\n",
        "\n",
        "        # Assuming you want to use the last output for prediction\n",
        "        last_output = transformer_output[:, -1, :]  # Select the last output for each batch\n",
        "\n",
        "        # Categorical outputs\n",
        "        output_pitch_type = self.output_linear_pitch_type(last_output)\n",
        "        output_event = self.output_linear_event(last_output)\n",
        "        output_type = self.output_linear_type(last_output)\n",
        "\n",
        "        # Continuous outputs\n",
        "        output_continuous = self.output_linear_continuous(last_output)\n",
        "\n",
        "        return output_pitch_type, output_event, output_type, output_continuous\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgx3jm8OjvXW"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F745FZZztzt4",
        "outputId": "e876c1fe-70bd-4406-968e-8496eb040316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "<ipython-input-20-b18207e02224>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_targets = torch.tensor(targets)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "num_classes_pitch_type = 18  # There are 12 'pitch_type_' columns\n",
        "num_classes_event = 30       # There are 50 'event_' columns\n",
        "num_classes_type = 3        # There are 17 'type_' columns\n",
        "\n",
        "\n",
        "# Convert the Pandas DataFrame to a PyTorch Tensor\n",
        "num_continuous = continuous_targets.shape[1]  # Number of continuous variables\n",
        "transformer_model = TransformerModel(input_size=len(feature_columns.columns),\n",
        "                                     num_classes_pitch_type=num_classes_pitch_type,\n",
        "                                     num_classes_event=num_classes_event,\n",
        "                                     num_classes_type=num_classes_type,\n",
        "                                     d_model=512,\n",
        "                                     nhead=8,\n",
        "                                     num_encoder_layers=6,\n",
        "                                     dim_feedforward=2048,\n",
        "                                     dropout=0.1)\n",
        "# Loss functions\n",
        "criterion_cat = nn.CrossEntropyLoss()  # For categorical targets\n",
        "criterion_cont = nn.MSELoss()  # For continuous targets\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "batch_size = 300000\n",
        "categorical_targets_tensor = torch.tensor(categorical_targets[:1].values).float()\n",
        "continuous_targets_tensor = torch.tensor(continuous_targets[0:1].values).float()\n",
        "#if categorical_targets_tensor.shape[0] == continuous_targets_tensor.shape[0] and continuous_targets.shape[1] == 2:\n",
        "  #    targets = torch.cat((categorical_targets_tensor, continuous_targets_tensor), dim=1)\n",
        "#else:\n",
        " #   raise ValueError(\"There is a mismatch in the shapes of the target tensors after handling null values.\")\n",
        "targets = categorical_targets_tensor\n",
        "val_features = torch.tensor(feature_columns[0:1].values).float()\n",
        "val_targets = torch.tensor(targets)\n",
        "\n",
        "# Split the data into batches\n",
        "for idx in range(int(feature_columns.shape[0]/batch_size)):\n",
        "  start_idx = idx * batch_size\n",
        "  end_idx = start_idx + batch_size\n",
        "\n",
        "  features = torch.tensor(feature_columns[start_idx:end_idx].values).float()\n",
        "\n",
        "  categorical_targets_tensor = torch.tensor(categorical_targets[start_idx:end_idx].values).float()\n",
        "  #continuous_targets_tensor = torch.tensor(continuous_targets[start_idx:end_idx].values).float()\n",
        "\n",
        "  # If the shapes are correct, concatenate them\n",
        "  #if categorical_targets_tensor.shape[0] == continuous_targets_tensor.shape[0] and continuous_targets.shape[1] == 2:\n",
        "     # targets = torch.cat((categorical_targets_tensor, continuous_targets_tensor), dim=1)\n",
        "    # targets = categorical_targets_tensor\n",
        " # else:\n",
        " #     raise ValueError(\"There is a mismatch in the shapes of the target tensors after handling null values.\")\n",
        "  targets = categorical_targets_tensor\n",
        "  # Split the data into training and validation sets\n",
        "  features_train, features_val, targets_train, targets_val = train_test_split(\n",
        "      features, targets, test_size=0.80, random_state=42)\n",
        "\n",
        "  val_features = torch.cat((val_features, features_val), dim=0)\n",
        "  val_targets = torch.cat((val_targets, targets_val), dim=0)\n",
        "\n",
        "  # Create a custom dataset class\n",
        "  # Number of categorical targets is the sum of unique classes for 'pitch_type', 'event', and 'type'\n",
        "  num_categorical = 51\n",
        "  # Create Dataset objects for training and validation sets\n",
        "  train_dataset = PitchDataset(features_train, targets_train, num_categorical)\n",
        "  val_dataset = PitchDataset(features_val, targets_val, num_categorical)\n",
        "\n",
        "  # Create DataLoaders for batching\n",
        "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "  num_classes = categorical_targets.shape[1]  # Number of categorical classes\n",
        "\n",
        "  # Training loop\n",
        "  # Training loop\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  print(f\"Using device: {device}\")\n",
        "  transformer_model = transformer_model.to(device)\n",
        "  for epoch in range(0):\n",
        "      transformer_model.train()  # Set the model to training mode\n",
        "      total_loss = 0\n",
        "\n",
        "\n",
        "      train_loader_tqdm = tqdm(train_loader)\n",
        "\n",
        "      for i, (inputs, (targets_cat_tuple, targets_cont)) in enumerate(train_loader_tqdm):\n",
        "          inputs, targets_cat_tuple, targets_cont = inputs.to(device), targets_cat_tuple.to(device), targets_cont.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs_pitch_type, outputs_event, outputs_type, outputs_cont = transformer_model(inputs)\n",
        "\n",
        "          # Convert the target tensors for categorical targets to Long\n",
        "          targets_pitch_type = targets_cat_tuple[:, 0].long()\n",
        "          targets_event = targets_cat_tuple[:, 1].long()\n",
        "          targets_type = targets_cat_tuple[:, 2].long()\n",
        "\n",
        "          # Calculate loss for pitch_type categorical target\n",
        "          loss_pitch_type = criterion_cat(outputs_pitch_type, targets_pitch_type)\n",
        "          # Calculate loss for event categorical target\n",
        "          loss_event = criterion_cat(outputs_event, targets_event)\n",
        "          # Calculate loss for type categorical target\n",
        "          loss_type = criterion_cat(outputs_type, targets_type)\n",
        "          # Calculate loss for continuous targets\n",
        "\n",
        "\n",
        "          # Combine losses\n",
        "          loss = loss_pitch_type + loss_event + loss_type\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backward pass and optimize\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the progress bar with the loss information\n",
        "          train_loader_tqdm.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "          train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "      # Calculate average loss for the epoch\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      print(f'Epoch [{epoch+1}/{5}] completed with average loss: {avg_loss:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihLNcYAqKge1"
      },
      "source": [
        "#Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2jR7qJWKlo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a643212-0d5e-49af-f243-d6cc535e841d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for pitch type: 0.0\n",
            "Accuracy for event: 0.11751201036662347\n",
            "Accuracy for type: 0.3895192103366236\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "transformer_model.eval()\n",
        "val_dataset = PitchDataset(val_features, val_targets, num_categorical)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "# Disable gradient computation since we are in inference mode\n",
        "with torch.no_grad():\n",
        "    predictions_cat = []\n",
        "    predictions_cont = []\n",
        "    true_labels_cat = []\n",
        "    true_labels_cont = []\n",
        "\n",
        "    for inputs, (targets_cat_tuple, targets_cont) in val_loader:  # Use your validation data loader\n",
        "        inputs = inputs.to(device)\n",
        "        targets_cat_tuple = targets_cat_tuple.to(device)\n",
        "        targets_cont = targets_cont.to(device)\n",
        "\n",
        "        # Get the model outputs\n",
        "        outputs_pitch_type, outputs_event, outputs_type, outputs_cont = transformer_model(inputs)\n",
        "\n",
        "        # Convert categorical predictions to probabilities and then to the predicted class\n",
        "        _, predicted_pitch_type = torch.max(F.softmax(outputs_pitch_type, dim=1), 1)\n",
        "        _, predicted_event = torch.max(F.softmax(outputs_event, dim=1), 1)\n",
        "        _, predicted_type = torch.max(F.softmax(outputs_type, dim=1), 1)\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        predictions_cat.extend(torch.cat((predicted_pitch_type.unsqueeze(1), predicted_event.unsqueeze(1), predicted_type.unsqueeze(1)), dim=1).cpu().numpy())\n",
        "        true_labels_cat.extend(targets_cat_tuple.cpu().numpy())\n",
        "\n",
        "        # For continuous targets, if they were normalized, apply inverse transformation\n",
        "        # Assuming `scaler` is your StandardScaler object used for normalization\n",
        "        #predicted_cont = scaler.inverse_transform(outputs_cont.cpu().numpy())\n",
        "       # true_cont = scaler.inverse_transform(targets_cont.cpu().numpy())\n",
        "\n",
        "       # predictions_cont.extend(predicted_cont)\n",
        "       # true_labels_cont.extend(true_cont)\n",
        "\n",
        "# Calculate accuracy for categorical targets\n",
        "predictions_cat = np.array(predictions_cat)\n",
        "true_labels_cat = np.array(true_labels_cat)\n",
        "accuracy_pitch_type = accuracy_score(true_labels_cat[:, 0], predictions_cat[:, 0])\n",
        "accuracy_event = accuracy_score(true_labels_cat[:, 1], predictions_cat[:, 1])\n",
        "accuracy_type = accuracy_score(true_labels_cat[:, 2], predictions_cat[:, 2])\n",
        "\n",
        "# Output the accuracy\n",
        "print(f'Accuracy for pitch type: {accuracy_pitch_type}')\n",
        "print(f'Accuracy for event: {accuracy_event}')\n",
        "print(f'Accuracy for type: {accuracy_type}')\n",
        "\n",
        "# For continuous targets, calculate some regression metric, like MSE\n",
        "predictions_cont = np.array(predictions_cont)\n",
        "true_labels_cont = np.array(true_labels_cont)\n",
        "#mse_start_speed = sklearn.metrics.mean_squared_error(true_labels_cont[:, 0], predictions_cont[:, 0])\n",
        "#mse_end_speed = sklearn.metrics.mean_squared_error(true_labels_cont[:, 1], predictions_cont[:, 1])\n",
        "\n",
        "# Output the regression metric\n",
        "#print(f'MSE for start speed: {mse_start_speed}')\n",
        "#print(f'MSE for end speed: {mse_end_speed}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MSbM6GxgnGQl"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer_model.state_dict(), 'model.pth')\n",
        "torch.save(transformer_model, 'model_full.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}